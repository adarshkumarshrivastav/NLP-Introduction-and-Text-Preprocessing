{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9OBd5PNlvG_",
        "outputId": "76e55206-6df9-4f83-96da-ac9ec01e2991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'word', 'tokenization', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# 1- How can you perform word tokenization using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import  word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is an example sentence for word tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2- How can you perform sentence tokenization using NLTK\n",
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"This is an example sentence. It contains multiple sentences.\"\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZTZReOWl9u6",
        "outputId": "526de225-12be-4f29-f66f-c07620631e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is an example sentence.', 'It contains multiple sentences.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3- How can you remove stopwords from a sentence\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(sentences):\n",
        "  token=word_tokenize(sentences)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_sentence = [w for w in token if not w.lower() in stop_words]\n",
        "  return filtered_sentence\n",
        "\n",
        "\n",
        "sentence = \"\"\"If you're visiting this page, you're likely here because you're searching for a random sentence. Sometimes a random word just isn't enough, and that is where the random sentence generator comes into play. By inputting the desired number, you can make a list of as many random sentences as you want or need. Producing random sentences can be helpful in a number of different ways. \"\"\"\n",
        "filtered_sentence = remove_stopwords(sentence)\n",
        "\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAfVcv0Gl9xW",
        "outputId": "5e96b175-6d16-4b96-d167-c78b49448a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'re\", 'visiting', 'page', ',', \"'re\", 'likely', \"'re\", 'searching', 'random', 'sentence', '.', 'Sometimes', 'random', 'word', \"n't\", 'enough', ',', 'random', 'sentence', 'generator', 'comes', 'play', '.', 'inputting', 'desired', 'number', ',', 'make', 'list', 'many', 'random', 'sentences', 'want', 'need', '.', 'Producing', 'random', 'sentences', 'helpful', 'number', 'different', 'ways', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How can you perform stemming on a word\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create a Porter Stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Define a word to stem\n",
        "print(\"__________________\")\n",
        "word = \"running\"\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_word = stemmer.stem(word)\n",
        "\n",
        "print(stemmed_word)  # Output: run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvbIndLgl9zx",
        "outputId": "23c04b0f-0179-401e-87c4-6e88a8b3cfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________\n",
            "run\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  How can you perform lemmatization on a word\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform lemmatization\n",
        "word = \"Running\"\n",
        "lemma = lemmatizer.lemmatize(word)\n",
        "\n",
        "print(lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-hfg8r_l92b",
        "outputId": "6176a5fc-8e6a-439e-c26a-975068f9ca49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 -  How can you normalize a text by converting it to lowercase and removing punctuation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub('['+string.punctuation+']', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "text = \"Hello, World! How's it going?\"\n",
        "normalized_text = normalize_text(text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "id": "6Ujlf3r8l94o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710e598a-6ad9-4cfb-b13d-62cd2bfa5531"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world hows it going\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7-  How can you create a co-occurrence matrix for words in a corpus\n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "\n",
        "def create_cooccurrence_matrix(corpus, window_size=2):\n",
        "    # Tokenize and preprocess the corpus\n",
        "    tokens = [word.lower() for word in corpus.split()]\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocabulary = list(set(tokens))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Initialize the matrix\n",
        "    vocab_size = len(vocabulary)\n",
        "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "\n",
        "    # Populate the matrix\n",
        "    for i, word in enumerate(tokens):\n",
        "        # Determine the context window\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        context = tokens[start:i] + tokens[i+1:end]\n",
        "\n",
        "        for other_word in context:\n",
        "            cooccurrence_matrix[vocab_index[word]][vocab_index[other_word]] += 1\n",
        "\n",
        "    return cooccurrence_matrix, vocabulary\n",
        "\n",
        "\n",
        "\n",
        "# Example corpus\n",
        "corpus = \"The quick brown fox jumps over the lazy dog. The dog was quick.\"\n",
        "matrix, vocab = create_cooccurrence_matrix(corpus)\n",
        "\n",
        "# Display the results...\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Co-occurrence Matrix:\")\n",
        "print(matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "j0HTcL3Dl97O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cadb047e-3ae7-45c3-c003-f4a9457f9727"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['fox', 'dog', 'over', 'quick.', 'dog.', 'quick', 'was', 'lazy', 'the', 'jumps', 'brown']\n",
            "Co-occurrence Matrix:\n",
            "[[0 0 1 0 0 1 0 0 0 1 1]\n",
            " [0 0 0 1 1 0 1 0 1 0 0]\n",
            " [1 0 0 0 0 0 0 1 1 1 0]\n",
            " [0 1 0 0 0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 1 2 0 0]\n",
            " [1 0 0 0 0 0 0 0 1 0 1]\n",
            " [0 1 0 1 0 0 0 0 1 0 0]\n",
            " [0 0 1 0 1 0 0 0 2 0 0]\n",
            " [0 1 1 0 2 1 1 2 0 1 1]\n",
            " [1 0 1 0 0 0 0 0 1 0 1]\n",
            " [1 0 0 0 0 1 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# : How can you apply a regular expression to extract all email addresses from a text\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_email_addresses(text):\n",
        "    \"\"\"\n",
        "    Extracts all email addresses from a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to extract email addresses from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted email addresses.\n",
        "    \"\"\"\n",
        "    # Regular expression pattern to match email addresses\n",
        "    pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "    # Find all matches of the pattern in the text\n",
        "    email_addresses = re.findall(pattern, text)\n",
        "\n",
        "    return email_addresses\n",
        "\n",
        "\n",
        "# Example usage\n",
        "text = \"Contact me at SuryanshChourasiya9993@gmail.com for more information.\"\n",
        "email_addresses = extract_email_addresses(text)\n",
        "\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in email_addresses:\n",
        "    print(email)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ekVBi7HQl99v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bd8d1d-1f0d-4b12-eda4-25740be97535"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "SuryanshChourasiya9993@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# : How can you perform word embedding using Word2Vec\n",
        "\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "sentences = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"Word2Vec is a popular technique.\",\n",
        "    \"It represents words as vectors.\",\n",
        "    # Add more sentences here...\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "BCfFrQj1l-AX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8856a7c9-6d67-41af-f2d1-18cc97b94f60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1)"
      ],
      "metadata": {
        "id": "AdltSoSol-DL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the word vector for a specific word\n",
        "word_vector = model.wv['example']\n",
        "\n",
        "# Compute the similarity between two words\n",
        "similarity = model.wv.similarity('example', 'sentence')\n",
        "\n",
        "print(f\"Word Vector: {word_vector}\")\n",
        "print(f\"Similarity: {similarity}\")"
      ],
      "metadata": {
        "id": "mYDTYDV5l-GB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ade07d-a195-4186-948b-5280171d4cb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Vector: [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
            "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
            " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
            "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
            " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
            "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
            "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
            "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
            "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
            " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
            " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
            " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
            " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
            " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
            "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
            " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
            " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
            " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
            "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
            " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
            " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
            "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
            " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
            "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
            "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n",
            "Similarity: -0.14454565942287445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "#  How can you use Doc2Vec to embed documents\n",
        "\n",
        "documents = [\n",
        "    \"This is an example document.\",\n",
        "    \"Doc2Vec is a popular technique.\",\n",
        "    \"It represents documents as vectors.\",\n",
        "\n",
        "]\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Prepare tagged documents\n",
        "tagged_documents = []\n",
        "for i, document in enumerate(documents):\n",
        "    tokens = word_tokenize(document.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tagged_documents.append(TaggedDocument(tokens, [i]))\n",
        "\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "# Train the Doc2Vec model using tagged documents\n",
        "model = Doc2Vec(tagged_documents, vector_size=100, window=5, min_count=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "33Rt8WJ8l-Ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240ce88e-2f5f-4238-d98c-ce253859ed60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the document vector for a specific document\n",
        "document_vector = model.infer_vector(tokenized_documents[0])\n",
        "\n",
        "print(f\"Document Vector: {document_vector}\")"
      ],
      "metadata": {
        "id": "By3d-Xqml-Lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422b5b37-3dcf-4a52-fd50-95efad795145"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Vector: [-2.0091657e-03 -4.2069246e-04  1.6459254e-04  2.9977339e-03\n",
            "  3.8319090e-03  1.9660019e-03  1.7960397e-04 -4.2669019e-03\n",
            " -9.9116319e-04 -1.7483237e-03  1.9438697e-03  3.0918012e-03\n",
            "  3.1804179e-03 -4.7748806e-03  4.0108690e-04 -4.6071382e-03\n",
            " -4.3823561e-03  1.9474913e-03  1.5656597e-03  4.7103604e-03\n",
            " -1.8685132e-03  2.4110994e-03 -4.2760759e-03  3.9977375e-03\n",
            "  3.0885218e-03 -3.7351253e-03  4.5689349e-03 -6.9842441e-05\n",
            " -4.1438732e-03  3.2020384e-03  2.1396691e-03 -3.2261342e-03\n",
            " -2.7701306e-05 -2.7850608e-03 -3.1294865e-03 -1.3278215e-03\n",
            " -4.4973223e-03  2.6314429e-03 -4.8071281e-03  2.5521265e-03\n",
            "  1.5982421e-03 -2.7526307e-03 -3.4276766e-03 -3.2692000e-03\n",
            "  3.1239316e-03 -2.4981136e-04 -1.3219453e-03  2.7516210e-03\n",
            "  5.9936830e-04  1.1040124e-03  5.6942273e-04  3.9878911e-03\n",
            " -2.0587354e-04  3.7459752e-03  2.0634721e-03  4.3965881e-03\n",
            "  3.4764674e-03 -1.2699403e-03 -1.3245206e-03  4.4343052e-03\n",
            " -1.3789298e-03  1.7159277e-03  1.1985934e-03 -2.5687625e-03\n",
            "  3.5247740e-05 -1.5690306e-03  2.0101913e-03  4.0642447e-03\n",
            " -1.0593910e-03  1.7206770e-03  6.1577861e-04 -2.6789426e-03\n",
            " -9.6227578e-04  2.5907757e-03 -3.4770553e-03  3.3458620e-03\n",
            "  3.7733279e-03  3.5155637e-03  3.5299230e-03 -4.3197908e-03\n",
            " -3.0707868e-03  2.4081545e-03  2.5005019e-03 -3.7844544e-03\n",
            " -3.7851147e-03 -4.2882347e-03  2.9754078e-03 -4.2375890e-03\n",
            " -1.4423998e-03 -3.5897540e-03  3.0653942e-03 -3.9062230e-03\n",
            " -2.2715556e-04 -5.7565328e-04 -2.7888825e-03  2.3180780e-03\n",
            " -4.6969149e-03  9.7064686e-04 -3.7529822e-03  9.8444238e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How can you perform part-of-speech tagging\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"This is an example sentence.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(f\"POS Tags: {pos_tags}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Yvt6Wy9rl-OZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8af29d-144f-4c28-ebfd-8e775b9610a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Finding Similarity between Two Sentences using Cosine Similarity\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "\n",
        "# Tokenize the sentences\n",
        "sentence1 = \"This is an example sentence.\"\n",
        "sentence2 = \"This sentence is an example too.\"\n",
        "\n",
        "tokens1 = word_tokenize(sentence1.lower())\n",
        "tokens2 = word_tokenize(sentence2.lower())\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec([tokens1, tokens2], vector_size=100, window=5, min_count=1)\n",
        "\n",
        "# Get the vector representations of the sentences\n",
        "vector1 = sum([model.wv[token] for token in tokens1])\n",
        "vector2 = sum([model.wv[token] for token in tokens2])\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "similarity = 1 - spatial.distance.cosine(vector1, vector2)\n",
        "\n",
        "print(f\"Cosine Similarity: {similarity}\")"
      ],
      "metadata": {
        "id": "RyMGoYVUl-Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5bff02-e0e8-46d3-b304-2d709858a5f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Named Entities from a Sentence\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the sentence\n",
        "sentence = \"Apple is a technology company founded by Steve Jobs.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract named entities\n",
        "named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "print(f\"Named Entities: {named_entities}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ExbjMGFUl-Te",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f71ecc-5066-4656-bc76-8cd13bd0c66f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('Apple', 'ORG'), ('Steve Jobs', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting a Large Document into Smaller Chunks of Text\n",
        "def split_document_into_chunks(document, chunk_size=100):\n",
        "    \"\"\"\n",
        "    Splits a large document into smaller chunks of text.\n",
        "\n",
        "    Args:\n",
        "        document (str): The large document to split.\n",
        "        chunk_size (int, optional): The size of each chunk. Defaults to 100.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    words = document.split()\n",
        "\n",
        "    current_chunk = []\n",
        "    current_chunk_size = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_chunk_size + len(word) > chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_chunk_size = len(word)\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_chunk_size += len(word)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "document = \"This is a large document that needs to be split into smaller chunks.\"\n",
        "chunks = split_document_into_chunks(document)\n",
        "\n",
        "print(f\"Chunks: {chunks}\")"
      ],
      "metadata": {
        "id": "tkzAbrONl-Vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ac1e82-e1ff-4877-f8eb-f63b35ed3810"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks: ['This is a large document that needs to be split into smaller chunks.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating TF-IDF for a Set of Documents\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load the 20 newsgroups dataset\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Fit the vectorizer to the dataset and transform the data into TF-IDF vectors\n",
        "tfidf_vectors = vectorizer.fit_transform(dataset.data)\n",
        "\n",
        "print(f\"TF-IDF Vectors Shape: {tfidf_vectors.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvYwa38_KI2z",
        "outputId": "9bc04e3f-1b1b-4ca6-985f-87d8c38e6188"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectors Shape: (11314, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Tokenization, Stopword Removal, and Stemming in One Go\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Applies tokenization, stopword removal, and stemming to the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]"
      ],
      "metadata": {
        "id": "U7lRw3a0KMVB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oq9W0EnhKI5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjAIJzngKI8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCP49-dxKI_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-N-wTcQKJB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mpix_J-PKJEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWGrG66QKJHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRlOoFG-KJLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dqsh-CANKJOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}